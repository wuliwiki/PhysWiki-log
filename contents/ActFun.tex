% 激活函数
% 激活函数

\pentry{函数（高中）\upref{functi}，}

\textbf{激活函数}（Activation function）是人工神经元计算流程的最后一步，跟随在仿射变换之后的一个非线性变换。神经元中的仿射变换是带有参数的，参数的值是模型训练时学习而来的。而激活函数往往是一个无参数的固定的非线性变换，它决定着一个神经元输出的值的范围。

神经网络中能够采用的激活函数种类繁多，往往须要根据实际应用场景做选择。设激活函数为$g$的输入为$x$，输出为$y$，有$y=g(x)$。


\subsection{恒等函数}
显然，恒等函数的表达式为：
\begin{equation}
y=g(x)=x~.
\end{equation}
函数图像为：
\begin{figure}[ht]
\centering
\includegraphics[width=8cm]{./figures/b4a7e4e745844640.png}
\caption{恒等函数} \label{fig_ActFun}
\end{figure}
导数为：
\begin{equation}
g'(x)=1~.
\end{equation}

\subsection{S型函数}
S型函数又称为Sigmoid函数。表达式为：
\begin{equation}
y=g(x)=\frac{e^x-1}{e^{-x}+1}~.
\end{equation}
函数图像为：
\begin{figure}[ht]
\centering
\includegraphics[width=8cm]{./figures/8693d64ffe26086f.png}
\caption{Sigmoid函数} \label{fig_ActFun2}
\end{figure}
其导数为：
\begin{equation}
g'(x)=\frac{g(x)}{1-g(x)}~.
\end{equation}

\subsection{双曲正切函数}
双曲正切函数表达式为：
\begin{equation}
y=g(x)=\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}~.
\end{equation}
函数图像为：
\begin{figure}[ht]
\centering
\includegraphics[width=8cm]{./figures/af28640f586a2823.png}
\caption{双曲正切函数} \label{fig_ActFun3}
\end{figure}
导数为：
\begin{equation}
g'(x)=1-g^2(x)~.
\end{equation}

\subsection{线性整流单元}
在现代深度神经网络的架构中，最常用的激活函数是\textbf{线性整流单元}（Rectified linear unit, ReLU）。其表达式为：
\begin{equation}
y=g(x)=\max\{0,x\}~.
\end{equation}
