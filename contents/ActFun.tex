% 激活函数
% keys 激活函数
% license Xiao
% type Tutor

\pentry{函数（高中）\nref{nod_functi}，神经网络\nref{nod_NN}，张量（向量与矩阵）\nref{nod_TsrFst}}{nod_34ec}

\textbf{激活函数}（Activation function）是人工神经元计算流程的最后一步，跟随在仿射变换之后的一个非线性变换。神经元中的仿射变换是带有参数的，参数的值是模型训练时学习而来的。而激活函数往往是一个无参数的固定的非线性变换，它决定着一个神经元输出的值的范围。

神经网络中能够采用的激活函数种类繁多，往往须要根据实际应用场景做选择。设激活函数为$g$的输入为$x$，输出为$y$，有$y=g(x)$。


\subsection{恒等函数}
显然，恒等函数的表达式为：
\begin{equation}
y=g(x)=x~.
\end{equation}
函数图像为：
\begin{figure}[ht]
\centering
\includegraphics[width=8cm]{./figures/b4a7e4e745844640.png}
\caption{恒等函数} \label{fig_ActFun}
\end{figure}
导数为：
\begin{equation}
g'(x)=1~.
\end{equation}

\subsection{阶跃函数}
阶跃函数的表达式为：
\begin{equation}
g'(x)=\leftgroup{1, x \geqslant 0 \\ 0, x < 0}~
\end{equation}
函数图像为：
\begin{figure}[ht]
\centering
\includegraphics[width=10cm]{./figures/702487dd34e9eaf6.png}
\caption{阶跃函数} \label{fig_ActFun_1}
\end{figure}
导数为：
\begin{equation}
g'(x)=\leftgroup{0, x \neq 0 \\ \text{不存在}, x = 0}~
\end{equation}


\subsection{S型函数}
S型函数又称为Sigmoid函数。表达式为：
\begin{equation}
%y=g(x)=\frac{e^x-1}{e^{-x}+1}~.
y=g(x)=\frac{1}{1+e^{-x}}~.
\end{equation}
函数图像为：
\begin{figure}[ht]
\centering
\includegraphics[width=12.5cm]{./figures/70774f32ad95eb4e.png}
\caption{Sigmiod函数图像} \label{fig_ActFun_2}
\end{figure}
其导数为：
\begin{equation}
% g'(x)=\frac{g(x)}{1-g(x)}~.
g'(x)=g(x)(1-g(x))~.
\end{equation}

\subsection{双曲正切函数}
双曲正切函数表达式为：
\begin{equation}
y=g(x)=\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}~.
\end{equation}
函数图像为：
\begin{figure}[ht]
\centering
\includegraphics[width=12cm]{./figures/af28640f586a2823.png}
\caption{双曲正切函数} \label{fig_ActFun3}
\end{figure}
导数为：
\begin{equation}
g'(x)=1-g^2(x)~.
\end{equation}

\subsection{线性整流单元}
在现代深度神经网络的架构中，最常用的激活函数是\textbf{线性整流单元}（Rectified linear unit, ReLU）。其表达式为：
\begin{equation}
y=g(x)=\max\{0,x\}~.
\end{equation}

在TensorFlow中，可以用以下函数实现线性整流单元：
\begin{lstlisting}[language=python]
tf.nn.relu(Tensor, name)
\end{lstlisting}

参数含义：
\begin{itemize}
\item Tensor： 输入张量
\item name：该操作的名称（可选）
\end{itemize}
返回值的数据类型为Tensor张量。

例子：
\begin{lstlisting}[language=python]
import tensorflow as tf

result = tf.nn.relu([2., -1., 0.]).numpy()
print(result)
\end{lstlisting}
执行后输出：
\begin{lstlisting}[language=python]
array([2., 0., 0.], dtype=float32)
\end{lstlisting}

从上述代码和执行结果可以看出，tf.nn.relu函数对于输入张量的每个分量做线性整流处理。输入张量$[2, -1, 0]$的三个分量中的$2$是大于$0$的，因此结果张量中的第$1$个分量为$2$；第$2$分量为$-1$，小于$0$，因此计算后的结果为$0$；第$3$个分量为$0$，计算结果显然为$0$。
