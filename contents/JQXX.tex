% 机器学习（综述）
% license CCBYSA3
% type Wiki

本文根据 CC-BY-SA 协议转载翻译自维基百科\href{https://en.wikipedia.org/wiki/Machine_learning}{相关文章}。

\textbf{机器学习（ML）}是人工智能的一个研究领域，关注于开发和研究能够从数据中学习并对未见过的数据进行泛化的统计算法，从而在没有明确指令的情况下执行任务。[1] 深度学习领域的进展使得神经网络在性能上超越了许多先前的方法。[2]

机器学习应用于许多领域，包括自然语言处理、计算机视觉、语音识别、电子邮件过滤、农业和医学。[3][4] 将机器学习应用于商业问题的领域被称为预测分析。

统计学和数学优化（数学编程）方法构成了机器学习的基础。数据挖掘是一个相关的研究领域，专注于通过无监督学习进行探索性数据分析（EDA）。[6][7]

从理论角度来看，可能大致正确（PAC）学习为描述机器学习提供了一个框架。
\subsection{历史}  
“机器学习”这一术语由IBM员工、计算机游戏和人工智能领域的先驱亚瑟·塞缪尔（Arthur Samuel）于1959年创造。[8][9] 在这一时期，“自我学习计算机”这个同义词也曾被使用。[10][11]

尽管最早的机器学习模型是在1950年代由亚瑟·塞缪尔发明的，该程序用于计算每方在跳棋中的获胜概率，但机器学习的历史可以追溯到几十年来人类对研究人类认知过程的渴望和努力。[12] 1949年，加拿大心理学家唐纳德·赫布（Donald Hebb）出版了《行为的组织》（The Organization of Behavior）一书，在书中他提出了通过神经元之间特定交互形成的理论神经结构。[13] 赫布关于神经元相互作用的模型为人工智能和机器学习算法在节点（或计算机用来传输数据的人工神经元）下如何工作奠定了基础。[12] 其他研究人类认知系统的学者也为现代机器学习技术做出了贡献，包括逻辑学家沃尔特·皮茨（Walter Pitts）和沃伦·麦卡洛克（Warren McCulloch），他们提出了早期的神经网络数学模型，旨在开发模拟人类思维过程的算法。[12]

到1960年代初，雷神公司（Raytheon）开发了一种实验性的“学习机器”，名为Cybertron，它采用打孔带存储，用于分析声纳信号、心电图和语音模式，使用的是基础的强化学习。它通过人工操作员/教师反复“训练”以识别模式，并配备了一个“错误”按钮，用于在做出错误决策时促使其重新评估。[14] 1960年代有关机器学习的代表性书籍之一是尼尔森（Nilsson）的《学习机器》一书，主要讨论了用于模式分类的机器学习。[15] 与模式识别相关的兴趣持续到1970年代，正如Duda和Hart在1973年所描述的那样。[16] 1981年，有报告讨论了使用教学策略，使人工神经网络学习从计算机终端识别40个字符（26个字母、10个数字和4个特殊符号）。[17]

汤姆·M·米切尔（Tom M. Mitchell）提出了机器学习领域算法的广泛引用的正式定义：“如果一个计算机程序在经验E的基础上，针对某些任务类别T，通过性能度量P，在T类别中的任务执行表现有所提高，则该计算机程序可以说是从经验E中学习。”[18] 这个关于机器学习所涉及任务的定义提供了一个基本的操作性定义，而不是从认知角度来定义该领域。这一概念沿袭了阿兰·图灵（Alan Turing）在其论文《计算机器与智能》中的提议，其中“机器能思考吗？”的问题被“机器能做我们（作为思维实体）能够做的事吗？”所取代。[19]

现代机器学习有两个目标。一是根据已开发的模型对数据进行分类；另一目的是根据这些模型对未来的结果进行预测。一个专门用于数据分类的假设性算法，可能会使用计算机视觉技术，结合监督学习来训练算法识别癌变的痣。用于股票交易的机器学习算法可能会向交易员提供未来潜在的预测。[20]
\subsection{与其他领域的关系}  
\subsubsection{人工智能}
\begin{figure}[ht]
\centering
\includegraphics[width=6cm]{./figures/108658dec604aeb3.png}
\caption{机器学习作为人工智能的一个子领域[21]} \label{fig_JQXX_1}
\end{figure}
作为一项科学事业，机器学习源自于对人工智能（AI）的探索。在人工智能作为学术学科的早期，一些研究者希望让机器从数据中学习。他们试图通过各种符号方法来解决这个问题，以及当时被称为“神经网络”的方法；这些方法大多是感知机和其他模型，后来被发现实际上是统计学中广义线性模型的再发明。[22] 概率推理也被应用，特别是在自动化医疗诊断中。[23]: 488

然而，随着对逻辑、知识为基础的方法的日益重视，人工智能与机器学习之间出现了分歧。概率系统面临数据获取和表示的理论和实践问题。[23]: 488 到1980年，专家系统已经主导了人工智能，统计学也不再受到青睐。[24] 虽然符号/知识为基础的学习方法在人工智能中仍然有所研究，推动了归纳逻辑编程（ILP）的发展，但更多的统计学研究现在已经脱离了人工智能的范畴，转向了模式识别和信息检索。[23]: 708–710, 755 神经网络研究在同一时期也被人工智能和计算机科学放弃。这一领域也被从其他学科的研究者所延续，包括约翰·霍普菲尔德、戴维·鲁梅哈特和杰弗里·辛顿。他们的主要成功是在1980年代中期重新发明了反向传播算法。[23]: 25

机器学习（ML）在1990年代重新组织并被认定为独立的学科，开始蓬勃发展。该领域的目标从实现人工智能转向解决具有实际性质的可解问题。它的重点从人工智能继承的符号方法转向了借鉴统计学、模糊逻辑和概率论的方法和模型。[24]
\subsubsection{数据压缩}  
机器学习与数据压缩之间有着紧密的联系。一个系统，如果能够根据整个历史数据预测序列的后验概率，就可以用于最优的数据压缩（通过对输出分布使用算术编码）。反过来，一个最优的压缩器也可以用于预测（通过找到在给定前历史数据的情况下最能压缩的符号）。这种等价性被用作使用数据压缩作为“通用智能”基准的理由。[25][26][27]  

另一种观点认为，压缩算法隐式地将字符串映射到隐式特征空间向量中，基于压缩的相似性度量计算这些特征空间中的相似性。对于每个压缩器 C(.)，我们定义一个关联的向量空间 ℵ，使得 C(.) 将输入字符串 x 映射到向量范数 ||~x||。由于空间限制，无法对所有压缩算法的特征空间进行详尽的检查；因此，选择检查三种典型的无损压缩方法：LZW、LZ77 和 PPM。[28]  

根据 AIXI 理论，这一点在 Hutter 奖中有更直接的解释，x 的最佳压缩是生成 x 的最小软件。例如，在该模型中，zip 文件的压缩大小包括 zip 文件和解压缩软件，因为没有这两者就无法解压，但可能存在一个更小的组合形式。  

由 AI 驱动的音频/视频压缩软件的例子包括 NVIDIA Maxine、AIVC。[29] 可以执行 AI 驱动图像压缩的软件的例子包括 OpenCV、TensorFlow、MATLAB 的图像处理工具箱（IPT）和高保真生成图像压缩。[30]  

在无监督机器学习中，K-means 聚类可以通过将相似的数据点分组成簇来压缩数据。该技术简化了处理缺乏预定义标签的大规模数据集，广泛应用于图像压缩等领域。[31]  

数据压缩旨在减少数据文件的大小，从而提高存储效率并加快数据传输速度。K-means 聚类是一种无监督机器学习算法，用于将数据集划分为指定数量的簇 k，每个簇由其点的质心表示。这个过程将大规模数据集压缩为一个更紧凑的代表性点集。特别在图像和信号处理方面，K-means 聚类通过将一组数据点替换为其质心，帮助减少数据量，同时大大减少所需的存储空间，保留原始数据的核心信息。[32]  

大语言模型（LLMs）也能实现无损数据压缩，正如 DeepMind 在其 Chinchilla 70B 模型中的研究所示。DeepMind 开发的 Chinchilla 70B 有效地压缩了数据，超越了传统方法，如便携式网络图形（PNG）图像和无损音频压缩编码（FLAC）音频。它成功地将图像和音频数据压缩至其原始大小的 43.4\% 和 16.4\%。[33]
\subsubsection{数据挖掘}  
机器学习和数据挖掘通常使用相同的方法，并且有很大的重叠，但尽管机器学习侧重于基于从训练数据中学习到的已知特征进行预测，数据挖掘则侧重于发现数据中（之前）未知的特征（这是数据库中知识发现的分析步骤）。数据挖掘使用了许多机器学习方法，但其目标不同；另一方面，机器学习也使用数据挖掘方法作为“无监督学习”或作为预处理步骤来提高学习者的准确性。这两个研究领域之间的许多混淆（尽管它们通常有独立的会议和期刊，ECML PKDD 是一个主要的例外）来自于它们所采用的基本假设：在机器学习中，性能通常是根据重现已知知识的能力来评估的，而在知识发现和数据挖掘（KDD）中，关键任务是发现以前未知的知识。在已知知识的评估中，未加信息的（无监督）方法通常会被其他监督方法超越，而在典型的 KDD 任务中，由于训练数据不可用，监督方法不能使用。

机器学习还与优化密切相关：许多学习问题被表述为在训练集示例上最小化某个损失函数。损失函数表示训练中模型的预测与实际问题实例之间的差异（例如，在分类中，人们希望为实例分配标签，而模型被训练来正确预测一组示例的预先分配标签）。[34]
\subsubsection{泛化}  
描述各种学习算法的泛化能力是当前研究的一个活跃话题，特别是对于深度学习算法。
\subsubsection{统计学}  
机器学习和统计学在方法上密切相关，但在主要目标上有所不同：统计学通过样本推断总体，而机器学习则发现可泛化的预测模式。[35] 根据迈克尔·I·乔丹（Michael I. Jordan）的说法，机器学习的思想，从方法论原则到理论工具，在统计学中有着悠久的历史。[36] 他还提出了“数据科学”这一术语，作为一个占位符来称呼整个领域。[36]

传统的统计分析需要事先选择一个最适合研究数据集的模型。此外，分析时仅包括基于以往经验认为重要或理论上相关的变量。相比之下，机器学习并不依赖于预先结构化的模型；相反，数据通过检测潜在的模式来塑造模型。使用更多的变量（输入）来训练模型，最终模型的准确性将更高。[37]

莱奥·布雷曼（Leo Breiman）区分了两种统计建模范式：数据模型和算法模型，[38] 其中“算法模型”指的就是像随机森林（Random Forest）这样的机器学习算法。

一些统计学家已经采纳了机器学习的方法，形成了他们称之为统计学习的结合领域。[39]
\subsubsection{统计物理}  
源自无序系统深厚物理学的分析和计算技术可以扩展到大规模问题，包括机器学习，例如，用于分析深度神经网络的权重空间。[40] 因此，统计物理学在医学诊断领域找到了应用。[41]
\subsection{理论}  
主要文章：计算学习理论和统计学习理论  
学习者的核心目标之一是从经验中进行泛化。[5][42] 在此语境下，泛化指的是学习机器在经历过学习数据集后，能够在新的、未见过的示例/任务上准确执行的能力。训练示例来自某些通常未知的概率分布（被认为代表事件发生的空间），学习者必须构建一个关于该空间的通用模型，使其能够在新的案例中做出足够准确的预测。

机器学习算法及其性能的计算分析是理论计算机科学的一个分支，称为计算学习理论，通过可能近似正确学习（PAC）模型进行研究。由于训练集是有限的，并且未来是不确定的，学习理论通常不能提供算法性能的保证。相反，通常会给出性能的概率边界。偏差-方差分解是量化泛化误差的一种方法。

为了在泛化的语境中获得最佳性能，假设的复杂性应与数据背后的函数复杂性相匹配。如果假设的复杂性低于函数的复杂性，则模型对数据拟合不足。如果响应中增加了模型的复杂性，则训练误差减少。但如果假设过于复杂，则模型容易过拟合，泛化能力会变差。[43]

除了性能边界，学习理论家还研究学习的时间复杂度和可行性。在计算学习理论中，如果某个计算可以在多项式时间内完成，那么它被认为是可行的。时间复杂度的结果有两种：正向结果表明某一类函数可以在多项式时间内学习；负向结果则表明某些类的函数无法在多项式时间内学习。