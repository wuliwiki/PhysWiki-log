% 大数定律（综述）
% license CCBYSA3
% type Wiki

本文根据 CC-BY-SA 协议转载翻译自维基百科\href{https://en.wikipedia.org/wiki/Law_of_large_numbers}{相关文章}。

在概率论中，大数定律是一条数学规律，它指出：在大量相互独立的随机样本中获得的结果的平均值将收敛于真实值（如果这个真实值存在）【1】。更正式地说，大数定律表明：对于一组独立同分布的样本，其样本均值将趋于真实的数学期望。

大数定律的重要性在于，它为某些随机事件的平均值提供了长期稳定的保证【1】【2】。例如，一个赌场在某次轮盘赌中可能会亏损，但在大量旋转后，其收益将趋近于一个可预测的百分比。任何玩家的连胜最终也会被游戏设定所“拉回”。需要注意的是，大数定律（顾名思义）只在观测次数足够大时才适用。并没有任何原则表明少数几次的观测结果就会接近期望值，或者说某种结果的连发会立即被其他结果“平衡”（参见赌徒谬误）。

大数定律仅适用于重复试验结果的平均值，并声称这个平均值会收敛到期望值；它并不意味着随着试验次数 $n$ 的增加，结果的总和一定会接近 $n$ 倍的期望值。

在其发展历程中，许多数学家对大数定律进行了不断完善。如今，大数定律被广泛应用于统计学、概率论、经济学以及保险学等多个领域【3】。
\subsection{示例}
\begin{figure}[ht]
\centering
\includegraphics[width=8cm]{./figures/614919ff753b49e7.png}
\caption{用一次掷单个骰子的特定实验来说明大数定律。随着该实验中掷骰次数的增加，所有结果数值的平均值逐渐趋近于 3.5。尽管在掷骰次数较少时（左侧）每次实验的曲线形状会有明显差异，但当掷骰次数增多时（右侧），这些曲线的形状将会极为相似。} \label{fig_DSdl_1}
\end{figure}
例如，掷一次六面骰子会得到 1、2、3、4、5 或 6 中的一个结果，每个数字出现的概率相等。因此，这次掷骰子的**期望值**是：
$$
\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = 3.5~
$$
根据大数定律，如果掷出大量的六面骰子，这些点数的平均值（有时称为样本均值）将趋近于 3.5，且随着掷骰数量的增加，结果的精确度也会提高。

大数定律还意味着，在一系列伯努利试验中，成功的经验概率会收敛于理论概率。对于伯努利随机变量，其期望值就是成功的理论概率，而 $n$ 个这样的变量的平均值（假设它们是独立同分布（i.i.d.）的）恰好就是相对频率。
\begin{figure}[ht]
\centering
\includegraphics[width=10cm]{./figures/35cb06686a7cb2ff.png}
\caption{这张图说明了相对频率向理论概率收敛的过程。假设从一个袋子中抽取红球的概率是 0.4，抽取黑球的概率是 0.6。左侧图显示的是抽到黑球的相对频率，右侧图显示的是抽到红球的相对频率，二者均基于 10,000 次试验。随着试验次数的增加，相对频率逐渐趋近于各自的理论概率，从而体现了大数定律的作用。} \label{fig_DSdl_2}
\end{figure}
例如，掷一枚公平硬币就是一次伯努利试验。当公平硬币被掷一次时，结果为正面的理论概率是 $\frac{1}{2}$。因此，根据大数定律，在进行“大量”掷硬币试验时，正面朝上的比例“应当”大约为 $\frac{1}{2}$。特别地，随着掷币次数 $n$ 趋近于无穷，正面朝上的比例将几乎必然地收敛到 $\frac{1}{2}$。

尽管正反面比例趋近于 $\frac{1}{2}$，正反面次数的绝对差值却几乎必然会随着试验次数增加而变大。也就是说，这个差值保持在一个很小的数值的概率会趋近于零。不过，这个差值与试验总次数的比例几乎必然趋近于零。从直觉上说，差值确实在增长，但它的增长速度比总次数慢。

另一个很好地体现大数定律的例子是蒙特卡洛方法。这是一大类依赖重复随机抽样以获得数值结果的计算算法。重复次数越多，得到的近似结果通常也越准确。该方法的重要性主要在于，有时候使用其他方法求解是非常困难甚至不可能的【4】。
\subsection{局限性}
在某些情况下，从大量试验中获得的结果的平均值可能不会收敛。例如，从柯西分布或某些 帕累托分布（Pareto distribution，α<1）中抽取的 $n$ 个结果的平均值不会随着 $n$ 增大而收敛，其原因在于这些分布具有重尾特性。[5] 柯西分布是一个没有数学期望的分布[6]，而当帕累托分布的参数 α 小于 1 时，其期望值为无穷大。[7]生成一个柯西分布的例子的方法之一是：令随机数等于一个在区间 $-90^\circ$ 到 $+90^\circ$ 内均匀分布的角度的正切值。[8] 这个分布的中位数为 0，但其数学期望不存在，并且 $n$ 个此类变量的平均值具有与其中任意一个变量相同的分布。因此，即使 $n$ 趋于无穷大，这个平均值在概率意义上不会收敛到 0 或任何其他值。

此外，如果试验中存在选择性偏差——例如人类的经济或理性行为中常见的偏差——即使增加试验次数，大数定律也无法消除这种偏差，偏差仍然会存在。
\subsection{历史}
\begin{figure}[ht]
\centering
\includegraphics[width=8cm]{./figures/2e2b2dc3ee5caed4.png}
\caption{扩散现象是大数法则的一个实例。一开始，容器中隔板（品红色线）左侧存在溶质分子，而右侧则没有。移除隔板后，溶质开始扩散，最终填满整个容器。顶部：当只有一个分子时，其运动看起来是完全随机的。中部：当分子数量增加时，虽然仍然存在随机波动，但可以明显看出溶质趋向于更加均匀地填满容器。底部：当溶质分子的数量极其庞大（多到无法看见）时，随机性基本消失：溶质似乎平滑而有规律地从高浓度区域流向低浓度区域。在现实情况下，尽管扩散的本质是随机的，化学家仍可以将其视为一种确定性的宏观现象（参见菲克定律）。} \label{fig_DSdl_3}
\end{figure}
意大利数学家杰罗拉莫·卡尔达诺（Gerolamo Cardano，1501–1576）曾在未加证明的情况下指出，经验统计的准确性随着试验次数的增加而提高。[9][3] 后来，这一观点被正式表述为“大数法则”。大数法则的一种特殊形式（适用于二元随机变量）最早由雅各布·伯努利证明。[10][3] 他花费了超过 20 年时间，才发展出足够严谨的数学证明，并于 1713 年在其著作《概率术》中发表。他将其称为“金色定理”，但后世更常称之为“伯努利定理”。这一定理不应与“伯努利原理”混淆，后者是以雅各布的侄子丹尼尔·伯努利的名字命名的。1837 年，西梅翁·德尼·泊松以“la loi des grands nombres”（法语，意为“大数法则”）的名称进一步描述了这一原理。[11][12][3] 此后，这一法则在文献中以这两个名称交替出现，但“law of large numbers”（大数法则）是最常用的名称。

在伯努利和泊松发表其成果之后，其他数学家也对大数法则进行了改进与拓展，其中包括切比雪夫（、马尔可夫、博雷尔、坎特利、柯尔莫哥洛夫和欣钦等人。[3] 马尔可夫证明了在某些较弱的条件下，大数法则仍适用于方差不存在的随机变量；1929 年，欣钦进一步证明，如果一组独立同分布的随机变量具有期望值，那么弱大数法则就成立。[14][15] 这些后续研究发展出了大数法则的两个主要形式：一种称为“弱大数法则”，另一种称为“强大数法则”，分别对应于样本均值趋近于期望值的两种不同收敛方式；特别地，如下文所解释的，强大数法则蕴含弱大数法则。[14]
\subsection{形式}
大数法则有两个不同的版本，称为强大数法则和弱大数法则【16】【1】。以下是它们在如下情形下的表述：设 $X_1, X_2, \dots$ 是一个无限序列的独立同分布（i.i.d.）的勒贝格可积随机变量，其期望满足：
$$
\mathbb{E}(X_1) = \mathbb{E}(X_2) = \cdots = \mu~
$$
这两个版本都断言样本平均值：
$$
\overline{X}_n = \frac{1}{n}(X_1 + \cdots + X_n)~
$$
收敛于期望值：
$$
\overline{X}_n \to \mu \quad \text{当 } n \to \infty~
$$
（其中“勒贝格可积”意味着 $\mathbb{E}(X_j)$ 按照勒贝格积分存在且有限；这并不意味着相关的概率测度对勒贝格测度绝对连续。）

入门级的概率教材通常还额外假设每个随机变量具有相同的有限方差，即：
$$
\operatorname{Var}(X_i) = \sigma^2 \quad \text{对所有 } i~
$$
并且各随机变量之间不相关。在这种情况下，$n$个随机变量的样本均值的方差为：
$$
\operatorname{Var}(\overline{X}_n) = \operatorname{Var}\left(\frac{1}{n}(X_1 + \cdots + X_n)\right) = \frac{1}{n^2} \operatorname{Var}(X_1 + \cdots + X_n) = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n}~
$$
这个公式可用于简化和缩短相关证明。但这种有限方差的假设并非必要。即使存在较大或无限的方差，只是会导致收敛速度变慢，大数法则依然成立【17】。

随机变量的相互独立性也可以被两两独立性【18】或可交换性【19】所取代，两种形式的大数法则仍然适用。

强大数法则和弱大数法则之间的区别在于所主张的收敛方式不同。关于这些收敛方式的解释，可参见“随机变量的收敛性”相关内容。
\subsubsection{弱大数法则}
\begin{figure}[ht]
\centering
\includegraphics[width=6cm]{./figures/d7530671329bae27.png}
\caption{模拟演示大数法则：每一帧中都会抛掷一枚硬币，其一面为红色，另一面为蓝色，并在对应的柱状图列中添加一个点。饼图显示当前红色与蓝色的比例。可以看到，虽然在初期比例波动较大，但随着试验次数的增加，比例逐渐趋近于 50\%。} \label{fig_DSdl_4}
\end{figure}
弱大数法则（也称为欣钦法则，Khinchin's law）指出：如果一组样本是从具有有限期望值的随机变量中独立同分布（i.i.d.）地抽取的，那么其样本均值将按概率收敛于该期望值：
$$
\overline{X}_n \xrightarrow{P} \mu \quad \text{当 } n \to \infty~
$$
也就是说，对于任意正数 ε，有：
$$
\lim_{n \to \infty} \Pr\left(|\overline{X}_n - \mu| < \varepsilon\right) = 1~
$$
解释这个结果：弱大数法则意味着，对于任意非零的容差（$\varepsilon$），无论多小，只要样本量足够大，样本均值以极高的概率落在该容差范围内，也就是说，会非常接近于期望值。

正如前面提到的，弱大数法则适用于 i.i.d. 随机变量的情形，但它也适用于其他一些情况。例如：序列中的每个随机变量的方差可以不同，只要它们的期望值保持不变。如果这些方差是有界的，那么该法则依然成立，这一点早在 1867 年就被切比雪夫证明过。

（如果期望值在序列中发生变化，我们可以将法则应用于每个变量与其期望值之间的平均偏差。此时法则表明，这一偏差按概率收敛于零。）事实上，只要前 $n$ 项的样本均值的方差在 $n \to \infty$ 时趋于零，切比雪夫的证明就成立【15】。例如，假设该序列中的每个随机变量都服从均值为 0、方差为 $2n/\log(n+1)$ 的高斯分布（正态分布）。虽然方差不是有界的，但每一步的样本均值仍服从正态分布（因为是正态变量的平均值）。此时，总和的方差是各项方差之和，渐近等于 $n^2 / \log n$，因此样本均值的方差渐近等于 $1 / \log n$，并趋于零。还有一些例子表明，即使期望值不存在，弱大数法则也可能适用。
\subsubsection{强大数法则}
强大数法则（也称为柯尔莫哥洛夫法则）指出：样本平均值几乎必然收敛于期望值[21]：
$$
\overline{X}_n \xrightarrow{\text{a.s.}} \mu \quad \text{当 } n \to \infty~
$$
也就是说：
$$
\Pr\left(\lim_{n \to \infty} \overline{X}_n = \mu\right) = 1~
$$
这意味着，当试验次数 $n$ 趋于无穷时，样本平均值收敛于期望值的概率为 1。相比弱大数法则，强大数法则的现代证明更为复杂，通常依赖于构造适当的子序列来完成证明【17】。

强大数法则本身可以看作是遍历定理的一个特例。这种观点为随机变量的期望值（在勒贝格积分意义下）提供了直观解释，即“长期平均”的含义。

此法则被称为“强”大数法则，是因为几乎处处收敛意味着依概率收敛；也就是说，强收敛蕴含弱收敛。然而，弱大数法则在某些强大数法则不适用的情形下仍然成立，这种情况下的收敛是仅仅弱的。参见“强弱大数法则的区别”。

强大数法则适用于具有期望值的独立同分布随机变量（如同弱大数法则）。这是由柯尔莫哥洛夫在1930年首次证明的。他在1933年进一步证明：如果一组变量是独立同分布的，那么要使平均值几乎处处收敛于某个数（这可以视为强大数法则的另一种表述），那么这些变量必须具有期望值（一旦期望存在，平均值就会几乎处处收敛于它）【22】。

如果各项是独立但不同分布的，那么仍然有：
$$
\overline{X}_n - \operatorname{E}[\overline{X}_n] \xrightarrow{\text{a.s.}} 0~
$$
前提是每个 $X_k$ 都具有有限的二阶矩，并且满足条件：
$$
\sum_{k=1}^{\infty} \frac{1}{k^2} \operatorname{Var}[X_k] < \infty~
$$
该陈述被称为柯尔莫哥洛夫强大数法则，可参见 Sen & Singer (1993, 定理 2.3.10)。
\subsubsection{弱大数法则与强大数法则的区别}
弱大数法则表明，对于某个足够大的 $n$，样本平均值 $\overline{X}_n$ 很有可能接近期望值 $\mu$【23】。因此，它允许存在无限多次 $|\overline{X}_n - \mu| > \varepsilon$ 的情形，尽管这种偏离会随着 $n$ 增大变得越来越罕见。（但不必然意味着对所有 $n$，都有 $|\overline{X}_n - \mu| \ne 0$）

而强大数法则则表明，这种情况几乎肯定不会发生。也就是说，对于任意 $\varepsilon > 0$，几乎处处地存在某个足够大的 $N$，使得当 $n > N$ 时，不等式：
$$
|\overline{X}_n - \mu| < \varepsilon~
$$
恒成立【24】。换句话说，从某个时刻开始，样本平均值将一直保持在期望值 $\mu$ 的 $\varepsilon$ 邻域内，几乎必然如此。这正是强大数法则的收敛更强的体现。

以下情况中，强大数法则不成立，但弱大数法则成立【25】【26】：
\begin{enumerate}
\item 设 $X$ 是一个参数为 1 的指数分布随机变量。随机变量$\frac{\sin(X)e^{X}}{X}$根据勒贝格积分的定义是没有期望值的，但如果采用条件收敛的方式，并将该积分解释为狄利克雷积分，即一个非正常的黎曼积分，我们可以得到：
$$
\mathbb{E}\left(\frac{\sin(X)e^{X}}{X}\right) = \int_{x=0}^{\infty} \frac{\sin(x)e^{x}}{x} e^{-x} \, dx = \frac{\pi}{2}~
$$
这说明虽然按照严格的勒贝格积分定义期望值不存在，但在较弱条件下（例如弱大数法则适用的情境中）仍可得出某种“期望”意义下的收敛结果。
\item 设 $X$ 是一个以 0.5 为成功概率的几何分布随机变量。随机变量$\frac{2^{X}(-1)^{X}}{X}$在常规意义下没有期望值，因为对应的无穷级数不是绝对收敛的。但如果使用条件收敛的方式，我们可以得到：
$$
\mathbb{E}\left(\frac{2^{X}(-1)^{X}}{X}\right) = \sum_{x=1}^{\infty} \frac{2^{x}(-1)^{x}}{x} \cdot 2^{-x} = -\ln(2)~
$$
\item 另外，若某个随机变量的累积分布函数为：
$$
\begin{cases}
1 - F(x) = \dfrac{e}{2x \ln(x)}, & x \geq e \\
F(x) = \dfrac{e}{-2x \ln(-x)}, & x \leq -e
\end{cases}~
$$
那么该随机变量没有期望值，但弱大数法则仍然成立【27】【28】。
\item 设 $X_k$ 是取值为 $\pm \sqrt{k/\log \log \log k}$ 的随机变量（从足够大的 $k$ 开始，以确保分母为正），且各取值的概率均为 $\frac{1}{2}$【22】。那么 $X_k$ 的方差为：$\frac{k}{\log \log \log k}$由于对 Kolmogorov 强大数法则来说，其判据中的部分和在 $k = n$ 时的渐进行为是：$\frac{\log n}{\log \log \log n}$这是无界的，因此 Kolmogorov 的强大数法则不适用。
如果我们将这些随机变量替换为具有相同方差的高斯变量（即均值为 0、方差为 $\frac{k}{\log \log \log k}$ 的正态分布），那么在任意时刻的平均值也将服从正态分布。此时其标准差的渐进行为为：$\frac{1}{\sqrt{2 \log \log \log n}}$随着 $n$ 增大，平均值的分布宽度趋于 0，但对于给定的 $\varepsilon$，总存在一个不趋于 0 的概率，表明在第 $n$ 次试验之后的某一时刻，平均值将重新达到 $\varepsilon$。由于分布宽度不为 0，因此存在一个正的下界 $p(\varepsilon)$，即在 $n$ 次试验后，平均值达到 $\varepsilon$ 的概率至少为 $p(\varepsilon)$。平均值在某个依赖于 $n$ 的 $m$ 次试验前达到 $\varepsilon$ 的概率至少为 $\frac{p(\varepsilon)}{2}$，而即使在 $m$ 次试验之后，平均值仍有至少 $p(\varepsilon)$ 的概率达到 $\varepsilon$。这似乎说明 $p(\varepsilon) = 1$，即平均值将以概率 1 无穷次地达到 $\varepsilon$。
\end{enumerate}
\subsubsection{一致大数法则}
一致大数法则是对经典大数法则的推广，适用于一类估计量的集合，其收敛在该集合上是一致的，因此得名“一致大数法则”。

设 $f(x, \theta)$ 是在 $\theta \in \Theta$ 上定义的某个函数，并且在 $\theta$ 上是连续的。那么对于任意固定的 $\theta$，序列 $\{f(X_1, \theta), f(X_2, \theta), \dots\}$ 将是一组独立同分布的随机变量，其样本均值按概率收敛于 $\mathbf{E}[f(X, \theta)]$。这称为对每个 $\theta$ 的逐点收敛。

而一致大数法则给出了收敛在整个 $\theta$ 空间内一致成立的条件。具体来说，如果满足以下条件【29】【30】：
\begin{enumerate}
\item $\Theta$ 是紧集；
\item $f(x, \theta)$ 对于几乎所有 $x$ 在每个 $\theta \in \Theta$ 处连续，且对每个 $\theta$ 来说是 $x$ 的可测函数；
\item 存在一个主导函数 $d(x)$，使得 $\mathbf{E}[d(X)] < \infty$；
并且对所有 $\theta \in \Theta$，都有：
   $$
   \| f(x, \theta) \| \leq d(x)~
   $$
\end{enumerate}
那么 $\mathbb{E}[f(X, \theta)]$ 在 $\theta$ 上是连续的，且有：
$$
\sup_{\theta \in \Theta} \left\| \frac{1}{n} \sum_{i=1}^{n} f(X_i, \theta) - \mathbb{E}[f(X, \theta)] \right\| \xrightarrow{P} 0~
$$
即，样本均值对期望的偏差在整个 $\Theta$ 上按概率一致收敛于 0。这个结果在推导极值估计量的相合性时非常有用。
\subsubsection{伯雷尔大数法则}
伯雷尔大数法则（以法国数学家埃米尔·伯雷尔 Émile Borel 命名）指出：如果一个实验在相同条件下独立重复进行大量次，那么某一特定事件发生的频率比例将趋近于该事件在单次试验中发生的概率；重复次数越多，这种近似就越准确。

更精确地说，设 $E$ 表示某个事件，$p$ 是该事件在一次试验中发生的概率，$N_n(E)$ 表示事件 $E$ 在前 $n$ 次试验中发生的次数，则有概率为 1（即“几乎必然”）地成立：
$$
\frac{N_n(E)}{n} \to p \quad \text{当} \quad n \to \infty.~
$$
这个定理为我们日常直觉中“概率就是长期频率”的想法提供了严格的数学基础。它是概率论中几种更一般形式的大数法则的一个特例。
\subsection{弱大数法则的证明}
设 $X_1, X_2, \ldots$ 是一个无限的独立同分布（i.i.d.）随机变量序列，其期望值有限，即
$$
E(X_1) = E(X_2) = \cdots = \mu < \infty,~
$$
我们关注样本均值的收敛性：
$$
\overline{X}_n = \frac{1}{n}(X_1 + \cdots + X_n).~
$$
弱大数法则陈述如下：
$$
\overline{X}_n \overset{P}{\longrightarrow} \mu \quad \text{当 } n \to \infty.~
$$
也就是说，样本均值在概率意义下收敛于期望值 $\mu$。
\subsubsection{使用切比雪夫不等式证明（假设方差有限}
此证明基于以下假设：每个随机变量 $X_i$ 都具有有限方差：$\operatorname{Var}(X_i) = \sigma^2 \quad \text{（对所有 } i \text{成立）}$由于这些随机变量是独立的，因此它们之间不相关。于是我们有：
$$
\operatorname{Var}(\overline{X}_n) = \operatorname{Var}\left(\frac{1}{n}(X_1 + \cdots + X_n)\right) = \frac{1}{n^2} \operatorname{Var}(X_1 + \cdots + X_n) = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n}~
$$
整个序列的公共期望值为 $\mu$，也是样本均值的期望值：
$$
E(\overline{X}_n) = \mu~
$$
在 $\overline{X}_n$ 上应用切比雪夫不等式得到：
$$
\operatorname{P}(|\overline{X}_n - \mu| \geq \varepsilon) \leq \frac{\sigma^2}{n\varepsilon^2}~
$$
由此可以得到：
$$
\operatorname{P}(|\overline{X}_n - \mu| < \varepsilon) = 1 - \operatorname{P}(|\overline{X}_n - \mu| \geq \varepsilon) \geq 1 - \frac{\sigma^2}{n\varepsilon^2}~
$$
随着 $n \to \infty$，上式趋近于 1。根据概率收敛的定义，我们由此得出：
$$
\overline{X}_n \overset{P}{\longrightarrow} \mu \quad \text{当 } n \to \infty~
$$
