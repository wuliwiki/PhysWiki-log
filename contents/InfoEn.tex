% 信息熵简介
% license Xiao
% type Tutor

\begin{issues}
\issueDraft
\end{issues}

给出一个 $N$ 元离散概率质量函数 $P(x_i)$（$i=1,\dots,N$），信息熵定义为
\begin{equation}\label{eq_InfoEn_1}
H = -\sum_{i=1}^N P(i) \log_2 P(x_i)~.
\end{equation}
为什么要这么定义？ 若每个可能都是等概率的， 那么 $P(x_i) = 1/N$， 所以上式化简为
\begin{equation}
H = \log_2 N~.
\end{equation}
若要从这 $N$ 种可能性种指明一种，所需要的信息量就是 $H$ 比特。

无论是在统计力学还是信息论种，简单来说，\textbf{熵代表不确定的程度}。在等可能的情况下，可能性更多，就越不确定。 若 $N$ 固定不变， 当各种可能的概率不完全相等时，根据\autoref{eq_InfoEn_1} 不难验证 $H < \log_2 N$。 这是因为 “知道哪种可能性更大” 本身也是一种信息。 就像在做选择题时，如果对这道题完全不会，那么每个选项对考生来说都是等可能的，但如果朦胧地记得一些信息，就可能会对某个选项更有把握，信息熵就会变小。
