% 深度学习 CNN 入门 3
% license Usr
% type Wiki


\pentry{卷积\nref{nod_Conv}，神经网络\nref{nod_NN}，全连接网络\nref{nod_FCNN}，Python 导航\nref{nod_PyFi}}{nod_567b}

在卷积神经网络中，对于输入的图像，需要多个不同的卷积核对其进行卷积，来提取这张图像不同的特征（多核卷积）；同时也需要多个卷积层进行卷积，来提取深层次的特征（深度卷积）。

感受野指的是卷积神经网络每一层输出的特征图(feature map)上每个像素点映射回输入图像上的区域大小。神经元感受野的范围越大表示其能接触到的原始图像范围就越大，也意味着它能学习更为全局，语义层次更高的特征信息；相反，范围越小则表示其所包含的特征越趋向局部和细节。因此感受野的范围可以用来大致判断每一层的抽象层次。并且我们可以很明显地知道网络越深，神经元的感受野越大。由此可知，深度卷积神经网络中靠前的层感受野较小，提取到的是图像的纹理、边缘等局部的、通用的特征；靠后的层由于感受野较大，提取到的是图像更深层次、更具象的特征。因此在迁移学习中常常会将靠前的层的参数冻结（不参与训练，因为他们在迁移到新的场景之前已经具备了提取通用特征的能力），来节省训练的时间和算力消耗。

在CNN中，卷积核的参数是共享的，即同一个卷积核在整个输入数据上进行滑动时，参数保持不变。这种权值共享的方式大大减少了模型的参数数量，降低了过拟合的风险，同时也提高了模型的训练效率。此外，权值共享还增强了模型对平移不变性的学习能力，使得CNN在处理平移、旋转等变换时具有更好的性能。

卷积层是CNN中最重要的组件之一，它负责提取输入数据的特征。在卷积层中，通过定义一组卷积核，对输入数据进行卷积运算，从而得到一系列的特征图。卷积操作能够有效捕捉局部特征和空间结构，实现对输入数据的特征提取和表示。\begin{figure}[ht]
\centering
\includegraphics[width=13cm]{./figures/efc15905abcb348d.png}
\caption{卷积层} \label{fig_CNN3_1}
\end{figure}

池化层用于对卷积层输出的特征图进行降维和抽样，以减少模型参数数量和计算复杂度。常见的池化操作包括最大池化和平均池化，通过在特定区域内取最大值或平均值来保留重要信息并减少数据量。池化操作还能增强模型对于平移和尺度变化的鲁棒性，提高模型的泛化能力。

\begin{figure}[ht]
\centering
\includegraphics[width=13cm]{./figures/b6f5f8ad58428409.png}
\caption{池化层} \label{fig_CNN3_2}
\end{figure}

全连接层通常位于卷积神经网络的最后几层，负责将卷积层和池化层提取的特征进行组合和分类。全连接层中的每一个神经元都与上一层的所有神经元相连，通过学习权重和偏置参数，实现对输入数据的高级特征学习和分类。


激活函数在CNN中扮演着非常重要的角色，它引入了非线性因素，使得神经网络能够学习复杂的非线性关系。常用的激活函数包括ReLU（Rectified Linear Unit）、Sigmoid和Tanh等。ReLU函数能够有效地缓解梯度消失问题，并加速网络的收敛；Sigmoid函数和Tanh函数则用于二分类和多分类问题，将输出值映射到[0, 1]或[-1, 1]之间，实现输出的归一化和概率化处理。
\begin{figure}[ht]
\centering
\includegraphics[width=13cm]{./figures/f65fdb50100fd0d5.png}
\caption{激活函数} \label{fig_CNN3_3}
\end{figure}
